{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- **기울기 소실(Gradient Vanishing)** : 입력층으로 갈수록 기울가가 점차 작아지며 가중치 업데이트가 제대로 이루어지지 않는 현상이다.\r\n",
    "- **기울기 폭주(Gradient Exploding)** : 기울기가 점차 커지며 가중치들이 비정상적으로 큰 값이 되고 결국 발산되기도 하는 현상이다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ReLU와 ReLU 변형"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Sigmoid 함수의 출력값이 0이나 1에 수렴하며 기울기가 0에 가까워지면 기울기 소실 문제가 발생하는 경우가 있다.\r\n",
    "- 기울기 소실을 방지하기 위해 은닉층에서는 시그모이드나 하이퍼볼릭탄젠트 함수 대신 ReLU나 ReLU 변형 함수(Leaky ReLU)를 사용한다.\r\n",
    "- ReLU는 죽은 ReLU 문제가 생길 수 있어 Leaky ReLU를 사용해 기울기가 0에 수렴하지 않도록 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 가중치 초기화 (Weight initialization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 세이비어 초기화 (Xavier Initialization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[세이비어 초기화 논문](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\r\n",
    "- 2010년 세이비어 글로럿과 요슈아 벤지오는 가중치 초기화가 모델에 미치는 영향을 분석하여 새로운 초기화 방법을 제안하였는데 이를 **세이비어 초기화(Xavier Initialization)** 또는 **글로럿 초기화(Clorot Initialization)** 라고 부른다.\r\n",
    "- 세이비어 초기화는 균등 분포(Uniform Distribution) 또는 정규 분포(Normal distribution) 로 초기화 하는 두가지 경우로 나뉜다.\r\n",
    "- 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 사용하여 식을 세운다. (이전 층 : n_in, 다음 층 : n_out)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 균등 분포"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 위 논문에서는 균등 분포를 사용하여 가중치를 초기화할 경우 다음과 같은 균등 분포 범위를 사용하라고 한다.\r\n",
    "![Xavier](../img/Xavier1.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 정규 분포로 초기화할 경우 평균이 0이고 표준편차 σ가 다음을 만족하도록 한다.\r\n",
    "![Xavier](../img/Xavier2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 세이비어 초기화는 **여러 층의 기울기 분산 사이에 균형을 맞춰** 특정 층이 너무 주목 받거나 뒤쳐지는 것을 막는다.\r\n",
    "- 세이비어 초기화는 S자 형태의 활성화 함수에 대해서는 성능이 좋지만, ReLU에 대해서는 성능이 그다지 좋지 않다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## He 초기화 (He Initialization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- ReLU나 ReLU의 변형 함수가 활성화 함수일 경우 세이비어 초기화보다는 He 초기화가 더 적절하다.\r\n",
    "[He 초기화 논문](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)\r\n",
    "- He 초기화 역시 정규 분포와 균등 분포 두 가지 경우로 나뉘지만 세이비어 초기화와 다르게 다음 층 뉴런의 수를 반영하지 않습니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 균등 분포로 초기화 할 경우 다음과 같은 균등 분포 범위를 가지도록 한다.\r\n",
    "![He](../img/He1.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 정규 분포로 초기화할 경우 표준 편차 σ가 다음을 만족하도록 한다.\r\n",
    "![He](../img/He2.png)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}