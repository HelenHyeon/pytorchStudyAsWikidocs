{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- **기울기 소실(Gradient Vanishing)** : 입력층으로 갈수록 기울가가 점차 작아지며 가중치 업데이트가 제대로 이루어지지 않는 현상이다.\r\n",
    "- **기울기 폭주(Gradient Exploding)** : 기울기가 점차 커지며 가중치들이 비정상적으로 큰 값이 되고 결국 발산되기도 하는 현상이다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ReLU와 ReLU 변형"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Sigmoid 함수의 출력값이 0이나 1에 수렴하며 기울기가 0에 가까워지면 기울기 소실 문제가 발생하는 경우가 있다.\r\n",
    "- 기울기 소실을 방지하기 위해 은닉층에서는 시그모이드나 하이퍼볼릭탄젠트 함수 대신 ReLU나 ReLU 변형 함수(Leaky ReLU)를 사용한다.\r\n",
    "- ReLU는 죽은 ReLU 문제가 생길 수 있어 Leaky ReLU를 사용해 기울기가 0에 수렴하지 않도록 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 가중치 초기화 (Weight initialization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 세이비어 초기화 (Xavier Initialization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[세이비어 초기화 논문](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\r\n",
    "- 2010년 세이비어 글로럿과 요슈아 벤지오는 가중치 초기화가 모델에 미치는 영향을 분석하여 새로운 초기화 방법을 제안하였는데 이를 **세이비어 초기화(Xavier Initialization)** 또는 **글로럿 초기화(Clorot Initialization)** 라고 부른다.\r\n",
    "- 세이비어 초기화는 균등 분포(Uniform Distribution) 또는 정규 분포(Normal distribution) 로 초기화 하는 두가지 경우로 나뉜다.\r\n",
    "- 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 사용하여 식을 세운다. (이전 층 : n_in, 다음 층 : n_out)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 균등 분포"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 위 논문에서는 균등 분포를 사용하여 가중치를 초기화할 경우 다음과 같은 균등 분포 범위를 사용하라고 한다.\r\n",
    "![Xavier](../img/Xavier1.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 정규 분포로 초기화할 경우 평균이 0이고 표준편차 σ가 다음을 만족하도록 한다.\r\n",
    "![Xavier](../img/Xavier2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 세이비어 초기화는 **여러 층의 기울기 분산 사이에 균형을 맞춰** 특정 층이 너무 주목 받거나 뒤쳐지는 것을 막는다.\r\n",
    "- 세이비어 초기화는 S자 형태의 활성화 함수에 대해서는 성능이 좋지만, ReLU에 대해서는 성능이 그다지 좋지 않다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## He 초기화 (He Initialization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- ReLU나 ReLU의 변형 함수가 활성화 함수일 경우 세이비어 초기화보다는 He 초기화가 더 적절하다.\r\n",
    "[He 초기화 논문](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)\r\n",
    "- He 초기화 역시 정규 분포와 균등 분포 두 가지 경우로 나뉘지만 세이비어 초기화와 다르게 다음 층 뉴런의 수를 반영하지 않습니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 균등 분포로 초기화 할 경우 다음과 같은 균등 분포 범위를 가지도록 한다.\r\n",
    "![He](../img/He1.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 정규 분포로 초기화할 경우 표준 편차 σ가 다음을 만족하도록 한다.\r\n",
    "![He](../img/He2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 배치 정규화 (Batch Normalization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 가중치 초기화를 사용하더라도 기울기 소실과 폭주는 언제든 발생할 수 있다.\r\n",
    "- **배치 정규화(Batch Normalization)** 는 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만든다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 내부 공변량 변화 (Internal Covariate Shift)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **내부 공변량 변화(Internal Covariate Shift)** : 학습 과정에서 **층 별로 입력 데이터 분포가 달라지는 현상** 을 의미한다.\r\n",
    "- 이전 층들의 학습에 의해서 이전 층이 학습했던 시점의 분포와 차이가 발생하게 된다. 관련 논문에서는 기울기의 소실과 폭주 등 딥 러닝 모델의 분안전성이 층마다 입력의 분포가 달라지기 때문이라 주장한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우를 의미한다.\r\n",
    "- 내부 공변량 변화는 신경망 층 사이에서 발생하는 입력 데이터의 분포 변화를 의미한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 배치 정규화 (Batch Normalization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **배치 정규화(Batch Normalization)** : 한 번에 들어오는 배치 단위로 정규화하는 것을 의미한다.\r\n",
    "- 배치 정규화는 각 층에서 활성화 함수를 통과하기 전에 수행된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 입력에 대해 평균을 0으로 만들어서 정규화를 한다.\r\n",
    "2. 정규화된 데이터에 대해서 스케일과 시프트를 수행한다.\r\n",
    "3. 이 때 두 개의 매기변수 **γ**(스케일에 사용)와 **β**(시프트에 사용)를 사용하는데 다음 레이어에 일정한 범위의 값들만 전달되게 한다.\r\n",
    "- 배치 정규화의 수식은 다음과 같다. (BN = 배치 정규화)\r\n",
    "![배치 정규화 수식](../img/BN.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 배치 정규화는 시그모이드, 하이퍼볼릭탄젠트 함수를 사용해도 기울기 소실 문제를 크게 개선시킨다.\r\n",
    "- 가중치 초기화에 훨씬 덜 민감해진다.\r\n",
    "- 훨씬 큰 학습률을 사용할 수 있어 학습 속도를 개선시킨다.\r\n",
    "- 미니 배치마다 평균과 표준편차를 계산하여 훈련 데이터에 일종의 잡음을 넣는 부수 효과로 과적합을 방지하는 효과도 있다. (하지만 드롭 아웃을 사용하는 것이 좋다.)\r\n",
    "- 모델을 복잡하게 하고, 추가 계산을 하는 것이라 테이스 데이터에 대한 예측 실행 시간은 느려진다.\r\n",
    "- 배치 정규화의 효과는 좋지만 내부 공변량 변화때문은 아니라는 이야기도 있다. [논문](https://arxiv.org/pdf/1805.11604.pdf)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 배치 정규화의 한계"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 미니 배치 크기에 의존적이다.\r\n",
    "    - 배치 크기가 너무 작을 경우 잘 동작하지 않아 훈련에 악영향을 줄 수 있다\r\n",
    "2. RNN에 적용이 어렵다.\r\n",
    "    - RNN은 각 시점(time step)마다 다른 통계치를 가지는데 이는 RNN에 배치 정규화를 적용하기 어렵게 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 층 정규화 (Layer Normalization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 배치 정규화의 시각화\r\n",
    "![배치 정규화](../img/BN1.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 층 정규화의 시각화\r\n",
    "![층 정규화](../img/LN.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 층 정규화는 배치 정규화와 다르게 배치 크기에 의존적이지 않고 RNN에도 적용하는 것이 수월하다."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}