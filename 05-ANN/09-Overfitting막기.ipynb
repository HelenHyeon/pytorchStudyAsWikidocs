{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 데이터 양 늘리기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 데이터 양을 늘릴 수록 모델이 일반적인 패턴을 학습하여 과적합을 방지할 수 있다.\r\n",
    "- Data Augmentation(데이터 증식, 증강) : 데이터 양이 적을 경우 의도적으로 기존의 데이터를 조금씩 변형하고 추가해 데이터 양을 늘리는 것."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 모델 복잡도 줄이기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 은닉층이나 매개변수의 수 등으로 인공 신경망의 복잡도가 달라진다.\r\n",
    "- 은닉층의 개수를 줄이는것과 같은 방법으로 인공 신경망의 복잡도를 줄일 수 있다.\r\n",
    "- 수용력(capacity) : 모델에 있는 매개변수들의 수"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. 가중치 규제(Regularization) 적용하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 가중치 규제는 복잡한 모델을 좀 더 간단하게 하는 방법이다.\r\n",
    "- L1 규제\r\n",
    "    - 가중치 w의 절대값 합계를 비용 함수에 추가한다.\r\n",
    "    - 모든 가중치에 대해 λ|w|를 기존의 비용 함수에 더한 값을 비용 함수로 한다.\r\n",
    "- L2 규제\r\n",
    "    - 모든 가중치 w의 제곱합을 비용 함수에 추가한다.\r\n",
    "    - 기존의 비용함수에 모든 가중치에 대해서 ½λw²를 더한 값을 비용함수로 한다.\r\n",
    "- λ : 규제의 강도를 정하는 하이퍼파라미터이며, 값이 클수록 모델이 훈련 데이터에 대해 적합한 매개 변수를 찾는 것보다 규제를 위해 추가된 항들을 작게 유지하는 것을 우선시 한다는 의미이다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- L1노름과 L2노름 둘 다 비용 함수를 최소화하기 위해서 가중치 w들의 값이 작아져야 한다는 특징이 있다.\r\n",
    "    - L1 규제\r\n",
    "        - 비용 함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에 가중치들의 절대값의 합 역시 최소가 되어야 한다.\r\n",
    "        - 가중치 w의 값은 0 또는 0과 가깝게 작아져야 하므로 어떤 특성들은 모델을 만들 때 거의 사용되지 않게 된다.\r\n",
    "        - Ex) H(x) = w₁x₁ + w₂x₂ + w₃x₃ + w₄x₄ 라는 식에서 w₃의 값이 0이 되면 x₃ 특성은 모델의 결과에 별 영향을 주지 못하는 특성임을 의미한다.\r\n",
    "        - 어떤 특성들이 모델이 영향을 주는지 판단할 때 유용하다.\r\n",
    "    - L2 규제\r\n",
    "        - 가중치들의 제곱을 최소화하므로 w의 값이 완전히 0이 되기보다 0에 가까워지는 경향이 있다.\r\n",
    "        - L2 노름은 가중치 감소(weight decay) 라고도 부른다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- PyTorch에서는 옵티마이저의 weight_decay 매개변수를 설정하여 L2 노름을 적용한다.\r\n",
    "- 기본값은 0이며 값을 직접 설정할 수 있다.\r\n",
    "```py\r\n",
    "model = Architecture1(10, 20, 2)\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Dropout (드롭아웃)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Overfitting](../img/Overfitting.png)\r\n",
    "- Dropout : 학습 과정에서 신경망의 일부를 사용하지 않는 방법이다.\r\n",
    "    - Ex) Dropout의 비율이 0.5인 경우 학습 과정마다 랜덤으로 절반의 뉴런만 사용한다.\r\n",
    "- 학습시에만 사용하고 예측에는 일반적으로 사용하지 않는다.\r\n",
    "- 학습 시 인공 신경망이 특정 뉴런이나 특정 조합에 의존적이게 되는 것을 방지한다."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit"
  },
  "interpreter": {
   "hash": "3019afab00293a1ae281e291b07de184983c2ba180a49ac4e0334f58b45935af"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}