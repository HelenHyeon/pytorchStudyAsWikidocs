{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 다중 선형 회귀\r\n",
    "- 단순 선형 회귀와 달리 x의 값이 여러개이다.\r\n",
    "- 3개인 경우 : H(x) = w₁x₁ + w₂x₂ + w₃x₃ + b"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch로 구현"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](../img/MLR.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "torch.manual_seed(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21ab689fed0>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\r\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\r\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\r\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# 가중치 w와 편향 b 초기화\r\n",
    "w1 = torch.zeros(1, requires_grad=True)\r\n",
    "w2 = torch.zeros(1, requires_grad=True)\r\n",
    "w3 = torch.zeros(1, requires_grad=True)\r\n",
    "b = torch.zeros(1, requires_grad=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# optimizer 설정\r\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "nb_epochs = 1000\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "\r\n",
    "    # H(x) 계산\r\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\r\n",
    "\r\n",
    "    # cost 계산\r\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\r\n",
    "\r\n",
    "    # cost로 H(x) 개선\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    # 100번마다 로그 출력\r\n",
    "    if epoch % 100 == 0:\r\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\r\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\r\n",
    "        ))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563634\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497607\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435026\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375730\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319511\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215696\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167818\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079378\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 벡터와 행렬 연산"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- x의 개수가 많아질수록 위의 방법처럼 일일이 선언하기 힘들다.\r\n",
    "- 행렬 곱셈(또는 벡터의 내적)을 이용하여 문제를 해결한다.\r\n",
    "- 벡터의 내적 : 행렬 곱셈 과정에서 이루어지는 벡터 연산\r\n",
    "![행렬곱셈 중 벡터의 내적 연산 과정](../img/MLR1.png)\r\n",
    "- 1*7 + 2*9 + 3*11 = 58"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 벡터 연산으로 표현"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> H(X) = w₁x₁ + w₂x₂ + w₃x₃\r\n",
    "### ![벡터의 내적](../img/MLR2.png)\r\n",
    "> H(X) = XW (두 벡터를 각각 X와 W로 표현할 경우)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 행렬 연산으로 표현"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ![](../img/MLR.png)\r\n",
    "- sample : 전체 훈련 데이터 개수를 셀 수 있는 단위 (예시 샘플의 개수 : 5개)\r\n",
    "- feature(특성) : 각 샘플의 y를 결정하는 독립변수 x (예시 특성의 개수 : 3개)\r\n",
    "- 독립변수 x의 개수 : sample * feature (5 * 3 = 15) => 행렬 X\r\n",
    "### ![행렬 X](../img/MLR3.png)\r\n",
    "- 벡터 W를 곱한다.\r\n",
    "### ![H(X) = XW](../img/MLR4.png)\r\n",
    "> H(X) = XW\r\n",
    "- 편향 b를 추가한다.\r\n",
    "### ![H(X) = XW + B](../img/MLR5.png)\r\n",
    "> H(X) = XW + B"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 행렬 연산으로 PyTorch 구현"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \r\n",
    "                               [93,  88,  93], \r\n",
    "                               [89,  91,  80], \r\n",
    "                               [96,  98,  100],   \r\n",
    "                               [73,  66,  70]]) # 5 * 3\r\n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]]) # 5 * 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# 가중치와 편향 선언\r\n",
    "W = torch.zeros((3, 1), requires_grad=True) # 3 * 1\r\n",
    "b = torch.zeros(1, requires_grad=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "hypothesis = x_train.matmul(W) + b # H(X) = XW + B"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# 완성\r\n",
    "# 데이터 선언 생략\r\n",
    "\r\n",
    "# optimizer 설정\r\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\r\n",
    "\r\n",
    "nb_epochs = 5000\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "\r\n",
    "    # H(x) 계산\r\n",
    "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\r\n",
    "    hypothesis = x_train.matmul(W) + b\r\n",
    "\r\n",
    "    # cost 계산\r\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\r\n",
    "\r\n",
    "    # cost로 H(x) 개선\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch % 100 == 0:\r\n",
    "        print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\r\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/5000 hypothesis: tensor([153.8917, 184.8801, 176.6712, 198.0892, 141.2973]) Cost: 3.906476\n",
      "Epoch  100/5000 hypothesis: tensor([153.8744, 184.8621, 176.7496, 198.0428, 141.3073]) Cost: 3.750101\n",
      "Epoch  200/5000 hypothesis: tensor([153.8570, 184.8449, 176.8258, 197.9976, 141.3176]) Cost: 3.600789\n",
      "Epoch  300/5000 hypothesis: tensor([153.8396, 184.8285, 176.8999, 197.9534, 141.3281]) Cost: 3.458147\n",
      "Epoch  400/5000 hypothesis: tensor([153.8220, 184.8129, 176.9720, 197.9101, 141.3388]) Cost: 3.321836\n",
      "Epoch  500/5000 hypothesis: tensor([153.8045, 184.7981, 177.0420, 197.8678, 141.3497]) Cost: 3.191587\n",
      "Epoch  600/5000 hypothesis: tensor([153.7869, 184.7839, 177.1102, 197.8264, 141.3609]) Cost: 3.067054\n",
      "Epoch  700/5000 hypothesis: tensor([153.7693, 184.7705, 177.1764, 197.7860, 141.3722]) Cost: 2.947954\n",
      "Epoch  800/5000 hypothesis: tensor([153.7517, 184.7577, 177.2408, 197.7465, 141.3836]) Cost: 2.834051\n",
      "Epoch  900/5000 hypothesis: tensor([153.7341, 184.7455, 177.3035, 197.7078, 141.3952]) Cost: 2.725066\n",
      "Epoch 1000/5000 hypothesis: tensor([153.7165, 184.7339, 177.3644, 197.6700, 141.4069]) Cost: 2.620798\n",
      "Epoch 1100/5000 hypothesis: tensor([153.6990, 184.7229, 177.4237, 197.6330, 141.4187]) Cost: 2.520976\n",
      "Epoch 1200/5000 hypothesis: tensor([153.6815, 184.7124, 177.4814, 197.5968, 141.4306]) Cost: 2.425423\n",
      "Epoch 1300/5000 hypothesis: tensor([153.6640, 184.7025, 177.5376, 197.5614, 141.4426]) Cost: 2.333911\n",
      "Epoch 1400/5000 hypothesis: tensor([153.6466, 184.6931, 177.5922, 197.5268, 141.4547]) Cost: 2.246307\n",
      "Epoch 1500/5000 hypothesis: tensor([153.6293, 184.6842, 177.6454, 197.4929, 141.4668]) Cost: 2.162368\n",
      "Epoch 1600/5000 hypothesis: tensor([153.6121, 184.6757, 177.6971, 197.4598, 141.4789]) Cost: 2.081943\n",
      "Epoch 1700/5000 hypothesis: tensor([153.5949, 184.6676, 177.7475, 197.4273, 141.4911]) Cost: 2.004880\n",
      "Epoch 1800/5000 hypothesis: tensor([153.5778, 184.6601, 177.7965, 197.3956, 141.5033]) Cost: 1.931007\n",
      "Epoch 1900/5000 hypothesis: tensor([153.5608, 184.6528, 177.8442, 197.3646, 141.5155]) Cost: 1.860193\n",
      "Epoch 2000/5000 hypothesis: tensor([153.5440, 184.6461, 177.8907, 197.3342, 141.5278]) Cost: 1.792310\n",
      "Epoch 2100/5000 hypothesis: tensor([153.5272, 184.6396, 177.9359, 197.3045, 141.5400]) Cost: 1.727174\n",
      "Epoch 2200/5000 hypothesis: tensor([153.5106, 184.6335, 177.9800, 197.2754, 141.5522]) Cost: 1.664746\n",
      "Epoch 2300/5000 hypothesis: tensor([153.4940, 184.6278, 178.0229, 197.2469, 141.5643]) Cost: 1.604849\n",
      "Epoch 2400/5000 hypothesis: tensor([153.4777, 184.6224, 178.0647, 197.2191, 141.5765]) Cost: 1.547356\n",
      "Epoch 2500/5000 hypothesis: tensor([153.4614, 184.6173, 178.1054, 197.1918, 141.5886]) Cost: 1.492202\n",
      "Epoch 2600/5000 hypothesis: tensor([153.4452, 184.6125, 178.1451, 197.1651, 141.6007]) Cost: 1.439279\n",
      "Epoch 2700/5000 hypothesis: tensor([153.4292, 184.6080, 178.1837, 197.1389, 141.6127]) Cost: 1.388471\n",
      "Epoch 2800/5000 hypothesis: tensor([153.4134, 184.6038, 178.2214, 197.1134, 141.6247]) Cost: 1.339690\n",
      "Epoch 2900/5000 hypothesis: tensor([153.3977, 184.5998, 178.2581, 197.0883, 141.6367]) Cost: 1.292868\n",
      "Epoch 3000/5000 hypothesis: tensor([153.3821, 184.5961, 178.2938, 197.0638, 141.6485]) Cost: 1.247914\n",
      "Epoch 3100/5000 hypothesis: tensor([153.3667, 184.5926, 178.3287, 197.0398, 141.6603]) Cost: 1.204711\n",
      "Epoch 3200/5000 hypothesis: tensor([153.3514, 184.5894, 178.3626, 197.0163, 141.6721]) Cost: 1.163247\n",
      "Epoch 3300/5000 hypothesis: tensor([153.3363, 184.5863, 178.3958, 196.9933, 141.6837]) Cost: 1.123409\n",
      "Epoch 3400/5000 hypothesis: tensor([153.3213, 184.5835, 178.4280, 196.9707, 141.6953]) Cost: 1.085128\n",
      "Epoch 3500/5000 hypothesis: tensor([153.3065, 184.5809, 178.4595, 196.9487, 141.7068]) Cost: 1.048353\n",
      "Epoch 3600/5000 hypothesis: tensor([153.2919, 184.5784, 178.4902, 196.9270, 141.7182]) Cost: 1.013023\n",
      "Epoch 3700/5000 hypothesis: tensor([153.2774, 184.5762, 178.5201, 196.9059, 141.7295]) Cost: 0.979054\n",
      "Epoch 3800/5000 hypothesis: tensor([153.2630, 184.5741, 178.5493, 196.8851, 141.7407]) Cost: 0.946419\n",
      "Epoch 3900/5000 hypothesis: tensor([153.2489, 184.5721, 178.5777, 196.8649, 141.7518]) Cost: 0.915040\n",
      "Epoch 4000/5000 hypothesis: tensor([153.2349, 184.5704, 178.6055, 196.8450, 141.7629]) Cost: 0.884885\n",
      "Epoch 4100/5000 hypothesis: tensor([153.2210, 184.5687, 178.6326, 196.8255, 141.7738]) Cost: 0.855894\n",
      "Epoch 4200/5000 hypothesis: tensor([153.2074, 184.5673, 178.6590, 196.8064, 141.7846]) Cost: 0.828021\n",
      "Epoch 4300/5000 hypothesis: tensor([153.1938, 184.5659, 178.6847, 196.7877, 141.7953]) Cost: 0.801214\n",
      "Epoch 4400/5000 hypothesis: tensor([153.1805, 184.5647, 178.7098, 196.7694, 141.8060]) Cost: 0.775432\n",
      "Epoch 4500/5000 hypothesis: tensor([153.1673, 184.5636, 178.7344, 196.7515, 141.8165]) Cost: 0.750649\n",
      "Epoch 4600/5000 hypothesis: tensor([153.1543, 184.5626, 178.7583, 196.7339, 141.8269]) Cost: 0.726811\n",
      "Epoch 4700/5000 hypothesis: tensor([153.1414, 184.5618, 178.7817, 196.7167, 141.8372]) Cost: 0.703884\n",
      "Epoch 4800/5000 hypothesis: tensor([153.1287, 184.5610, 178.8045, 196.6999, 141.8474]) Cost: 0.681825\n",
      "Epoch 4900/5000 hypothesis: tensor([153.1162, 184.5604, 178.8267, 196.6833, 141.8575]) Cost: 0.660611\n",
      "Epoch 5000/5000 hypothesis: tensor([153.1038, 184.5598, 178.8484, 196.6672, 141.8675]) Cost: 0.640194\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit"
  },
  "interpreter": {
   "hash": "3019afab00293a1ae281e291b07de184983c2ba180a49ac4e0334f58b45935af"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}