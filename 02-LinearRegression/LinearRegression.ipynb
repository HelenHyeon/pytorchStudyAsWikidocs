{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data 이해하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training/Test Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- training dataset : 결과 예측을 위해 사용하는 앞서 나온 데이터\r\n",
    "- test dataset : 학습이 끝난 모델의 성능을 테스트하는 데이터셋"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 훈련 데이터셋 구성\r\n",
    "- 훈련 데이터셋은 텐서의 형태를 가진다.\r\n",
    "- 입력과 출력 데이터를 각 텐서에 나누어 저장한다.\r\n",
    "- 보편적으로 입력은 x, 출력은 y를 사용한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\r\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\r\n",
    "print(x_train, '\\n', y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]]) \n",
      " tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 가설(Hypothesis)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Hypothesis : 머신러닝에서 식을 세우는 것\r\n",
    "- 선형회귀의 가설\r\n",
    "> ### y = Wx + b\r\n",
    "> ### H(x) = Wx + b\r\n",
    "- 가중치 : W(x와 곱해지는 수) => 기울기\r\n",
    "- 편향 : b => y절편\r\n",
    "[직선의 방정식](https://mathbang.net/443)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 비용 함수(Cost Funtion)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 비용함수 : 원래의 값과 가장 오차가 작은 가설함수를 도출하기 위해 사용되는 함수이다.\r\n",
    "- **(실제값 - 예측값)^2** 의 평균과 같다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ex)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ![그래프](../img/LR1.png)\r\n",
    "### 화살표 ↕ : 각 실제 값에 대한 오차 (실제값과 예측값의 차이)\r\n",
    "### ![예측값과 실제값의 오차](../img/LR2.png)\r\n",
    "### ![수식](../img/LR3.png)\r\n",
    "### 평균 제곱 오차를 W와 b에 의한 비용 함수로 재정의\r\n",
    "### ![수식](../img/LR4.png)\r\n",
    "### cost(W, b) 가 최소가 되는 W와 b를 구하여 훈련데이터를 가장 잘 표현하는 직선을 구할 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gradient Descent(경사 하강법)\r\n",
    "- Optimizer(옵티마이저) 알고리즘 : 위의 Cost Funtion의 값을 최소화 하는 W와 b를 찾는 알고리즘\r\n",
    "- Gradient Descent(경사 하강법) : 가장 기본적인 옵티마이저 알고리즘"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Descent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- b의 값은 0이라고 가정하고 설명한다. (y = Wx, cost(W))\r\n",
    "- 기울기 W의 값이 너무 크거나 반대로 너무 작으면 오차가 커진다. 이에 따라 W와 cost(W)의 관계는 다음과 같이 나타낼 수 있다.\r\n",
    "### ![W와 cost(W)의 관계](../img/Gradient1.png)\r\n",
    "- 위 그래프에서 맨 아래 볼록한 부분의 W값을 찾는 것이 목표이다.\r\n",
    "### ![접선의 기울기](../img/Gradient.png)\r\n",
    "- 순간 변화율이나 접선에서의 기울기 개념을 사용해 접선의 기울기가 0이 되는 지점을 찾는다.\r\n",
    "- 현재 W의 접선의 기울기를 구해 접선의 기울기가 0에 가까워지도록 작업을 반복한다.\r\n",
    "### ![수식](../img/Gradient2.png)\r\n",
    "- 현재 W의 접선의 기울기에 특정 숫자 α를 곱한 값을 빼서 새로운 W로 사용한다.\r\n",
    "### ![수식](../img/Gradient3.png)\r\n",
    "- 접선의 기울기가 음수이거나 양수일 경우 모두 접선의 기울기가 0인 방향으로 W의 값을 조정한다.\r\n",
    "- α의 의미 : W의 값을 변경할 때 얼마나 크게 변경하는지 결정한다. 너무 크거나 너무 작지 않은 적당한 값을 찾는것이 중요하다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 선형 회귀 구현"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "torch.manual_seed(1) # 재실행시 같은 결과가 나오도록 랜덤 시드 설정"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2a828038bb0>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\r\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 1])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(x_train.shape)\r\n",
    "x_train"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 1])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(y_train.shape)\r\n",
    "y_train"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 1])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[2.],\n",
       "        [4.],\n",
       "        [6.]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "W = torch.zeros(1, requires_grad=True) # 가중치 W를 0으로 초기화. 값이 변경될 것을 명시\r\n",
    "W"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "b = torch.zeros(1, requires_grad=True) # 편향 b를 0으로 초기화.\r\n",
    "b"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# 가설 세우기\r\n",
    "hypothesis = x_train*W*b # H(x) = Wx + b\r\n",
    "hypothesis"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# 비용함수 선언\r\n",
    "cost = torch.mean((hypothesis-y_train)**2) # 평균 구하기\r\n",
    "cost"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(18.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# 경사하강법\r\n",
    "optimizer = optim.SGD([W, b], lr=0.01) # SGD:경사하강법 일종, lr:학습률"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "optimizer.zero_grad() # gradient 0으로 초기화 (새로운 가중치, 편향에 대한 새로운 기울기를 구하기 위함)\r\n",
    "cost.backward() # 비용함수 미분 -> gradient 계산 (기울기 계산)\r\n",
    "optimizer.step() # 기울기에 0.01(학습률) 곱하여 W, b 업데이트"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### optimizer.zero_grad()\r\n",
    "- pytorch는 미분으로 얻은 기울기를 계속해 누적시키는 특징이 있기 때문에 zero_grad()를 이용해 초기화를 시킬 필요가 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# 완성\r\n",
    "\r\n",
    "# 데이터\r\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\r\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\r\n",
    "\r\n",
    "# 모델 초기화\r\n",
    "W = torch.zeros(1, requires_grad=True)\r\n",
    "b = torch.zeros(1, requires_grad=True)\r\n",
    "\r\n",
    "# optimizer 설정\r\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\r\n",
    "\r\n",
    "# epochs : 전체 훈련 데이터가 학습에 한 번 사용된 주기\r\n",
    "nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "\r\n",
    "    # H(x) 계산\r\n",
    "    hypothesis = x_train * W + b\r\n",
    "\r\n",
    "    # cost 계산\r\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\r\n",
    "\r\n",
    "    # cost로 H(x) 개선\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    # 100번마다 로그 출력\r\n",
    "    if epoch % 100 == 0:\r\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\r\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\r\n",
    "        ))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/2000 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/2000 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/2000 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/2000 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/2000 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/2000 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n",
      "Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 자동미분\r\n",
    "- 모델이 복잡할수록 직접 경사 하강법을 구현하기 어려워진다.\r\n",
    "- 파이토치의 자동미분을 이용해 손쉽게 경사하강법을 구현할 수 있다.\r\n",
    "- 임의로 2w^2 + 5 라는 식으로 실습"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "w = torch.tensor(2.0, requires_grad=True) # 값이 2인 스칼라 텐서\r\n",
    "w"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "y = w**2\r\n",
    "z = 2*y+5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "z.backward() # w에 대한 기울기 계산"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit"
  },
  "interpreter": {
   "hash": "3019afab00293a1ae281e291b07de184983c2ba180a49ac4e0334f58b45935af"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}