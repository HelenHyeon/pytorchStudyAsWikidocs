{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- **토치텍스트(Torchtext)** : PyTorch에서 텍스트에 대한 여러 추상화 기능을 제공하는 자연어 처리 라이브러리\r\n",
    "- 토치텍스트의 기능\r\n",
    "    - 파일 로드(Flie Loading) : 다양한 포맷의 코퍼스 로드\r\n",
    "    - 토큰화(Tokenization) : 단어 단위로 문장 분리\r\n",
    "    - 단어 집합(Vocab) : 단어 집합 생성\r\n",
    "    - 정수 인코딩(Integer encoding) : 전체 코퍼스의 단어들을 각각의 고유한 정수로 맵핑\r\n",
    "    - 단어 벡터(Word Vector) : 단어 집합의 단어들에 고유한 임베딩 벡터를 만든다. 랜덤값으로 초기화한 값이거나 사전 훈련된 임베딩 벡터를 로드할 수도 있다.\r\n",
    "    - 배치화(Batching) : 훈련 샘플들의 배치를 만든다. 이 과정에서 패딩(Padding) 작업도 이루어진다.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 훈련 데이터, 테스트 데이터 분리"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import urllib.request\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# IMDB 리뷰 데이터 다운로드\r\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x18a822d64f0>)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "print('전체 샘플의 개수 : {}'.format(len(df)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "전체 샘플의 개수 : 50000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "train_df = df[:25000]\r\n",
    "test_df = df[25000:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "train_df.to_csv(\"train_data.csv\", index=False)\r\n",
    "test_df.to_csv(\"test_data.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 필드 정의 (torchtext.data)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 필드를 통해 앞으로 어떤 전처리를 할 것인지 정의한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from torchtext.legacy import data # torchtext.data 임포트\r\n",
    "\r\n",
    "# 필드 정의\r\n",
    "TEXT = data.Field(sequential=True,\r\n",
    "                  use_vocab=True,\r\n",
    "                  tokenize=str.split,\r\n",
    "                  lower=True,\r\n",
    "                  batch_first=True,\r\n",
    "                  fix_length=20)\r\n",
    "\r\n",
    "LABEL = data.Field(sequential=False,\r\n",
    "                   use_vocab=False,\r\n",
    "                   batch_first=False,\r\n",
    "                   is_target=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- TEXT : 실제 테스트를 위한 객체\r\n",
    "- LABEL : 레이블 데이터를 위한 객체\r\n",
    "\r\n",
    "- 각 인자에 대한 설명\r\n",
    "    - sequential : 시퀀스 데이터 여부. (True가 기본값)\r\n",
    "    -use_vocab : 단어 집합을 만들 것인지 여부. (True가 기본값)\r\n",
    "    -tokenize : 어떤 토큰화 함수를 사용할 것인지 지정. (string.split이 기본값)\r\n",
    "    - lower : 영어 데이터를 전부 소문자화한다. (False가 기본값)\r\n",
    "    - batch_first : 미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부. (False가 기본값)\r\n",
    "    - is_target : 레이블 데이터 여부. (False가 기본값)\r\n",
    "    - fix_length : 최대 허용 길이. 이 길이에 맞춰서 패딩 작업(Padding)이 진행된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 데이터셋 만들기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from torchtext.legacy.data import TabularDataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- TabularDataset은 데이터를 불러오면 필드에서 정의한 토큰화 방법으로 토큰화를 수행하며 소문자화 같은 기본적이 전처리가 함께 이루어진다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "train_data, test_data = TabularDataset.splits(\r\n",
    "        path='.', train='train_data.csv', test='test_data.csv', format='csv',\r\n",
    "        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- path : 파일이 위치한 경로.\r\n",
    "- format : 데이터의 포맷.\r\n",
    "- fields : 위에서 정의한 필드를 지정. 첫번째 원소는 데이터 셋 내에서 해당 필드를 호칭할 이름, 두번째 원소는 지정할 필드.\r\n",
    "- skip_header : 데이터의 첫번째 줄은 무시."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "print('훈련 샘플의 개수 : {}'.format(len(train_data)))\r\n",
    "print('테스트 샘플의 개수 : {}'.format(len(test_data)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "훈련 샘플의 개수 : 25000\n",
      "테스트 샘플의 개수 : 25000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "print(vars(train_data[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'text': ['my', 'family', 'and', 'i', 'normally', 'do', 'not', 'watch', 'local', 'movies', 'for', 'the', 'simple', 'reason', 'that', 'they', 'are', 'poorly', 'made,', 'they', 'lack', 'the', 'depth,', 'and', 'just', 'not', 'worth', 'our', 'time.<br', '/><br', '/>the', 'trailer', 'of', '\"nasaan', 'ka', 'man\"', 'caught', 'my', 'attention,', 'my', 'daughter', 'in', \"law's\", 'and', \"daughter's\", 'so', 'we', 'took', 'time', 'out', 'to', 'watch', 'it', 'this', 'afternoon.', 'the', 'movie', 'exceeded', 'our', 'expectations.', 'the', 'cinematography', 'was', 'very', 'good,', 'the', 'story', 'beautiful', 'and', 'the', 'acting', 'awesome.', 'jericho', 'rosales', 'was', 'really', 'very', 'good,', \"so's\", 'claudine', 'barretto.', 'the', 'fact', 'that', 'i', 'despised', 'diether', 'ocampo', 'proves', 'he', 'was', 'effective', 'at', 'his', 'role.', 'i', 'have', 'never', 'been', 'this', 'touched,', 'moved', 'and', 'affected', 'by', 'a', 'local', 'movie', 'before.', 'imagine', 'a', 'cynic', 'like', 'me', 'dabbing', 'my', 'eyes', 'at', 'the', 'end', 'of', 'the', 'movie?', 'congratulations', 'to', 'star', 'cinema!!', 'way', 'to', 'go,', 'jericho', 'and', 'claudine!!'], 'label': '1'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# 필드 구성 확인.\r\n",
    "print(train_data.fields.items())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_items([('text', <torchtext.legacy.data.field.Field object at 0x0000018A57A3C5E0>), ('label', <torchtext.legacy.data.field.Field object at 0x0000018A57A3CFA0>)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vocabulary 만들기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "TEXT.build_vocab(train_data, min_freq=10, max_size=10000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "print('단어 집합의 크기 : {}'.format(len(TEXT.vocab)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "단어 집합의 크기 : 10002\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(TEXT.vocab.stoi)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 단어 집합의 크기를 10000개로 제한했지만 실제로는 0~10001번까지 총 10002개인 이유는 토치텍스트에서 임의로 특별 토큰 **<unk\\>** 와 **<pad\\>** 를 추가했기 때문이다.\r\n",
    "- <unk\\> : 집합에 없는 단어 표현\r\n",
    "- <pad\\> : 길이 맞추는 패딩 작업"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 토치텍스트의 데이터로더 만들기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "from torchtext.legacy.data import Iterator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "batch_size = 5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "train_loader = Iterator(dataset=train_data, batch_size = batch_size)\r\n",
    "test_loader = Iterator(dataset=test_data, batch_size = batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))\r\n",
    "print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "훈련 데이터의 미니 배치 수 : 5000\n",
      "테스트 데이터의 미니 배치 수 : 5000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "batch = next(iter(train_loader)) # 첫번째 미니배치"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "print(type(batch))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torchtext.legacy.data.batch.Batch'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 일반적 데이터로더 : 미니 배치를 텐서로 가져온다.\r\n",
    "- 토치텍스트 데이터로더 : torchtext.data.batch.Batch 객체를 가져온다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "print(batch.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1959,    0,    0,    0,   41,   21,  677,  556,    4,   21,  397,    0,\n",
      "         2935,    8,    3,  426,    0,  848,    0,    0,    0,  436,    6,  328,\n",
      "          101,  143,    2, 2397, 9248,    0,   97,    0,  607,  368, 6377,   17,\n",
      "            3,  161,    0,    0, 3226,    4,    0,    6,    3,    0,   27,    7,\n",
      "          552,   10],\n",
      "        [  10,    7,    3,   20,   17,    3,  422, 7483,   18,   49,  953, 5003,\n",
      "           11,  133,   99,   26,   71,  685,   17,   47,   29,  225,  278,   51,\n",
      "            0,    0,   11,  178,    0,    2,   77,    7,  105,    0, 1630, 1691,\n",
      "            4,   52,   23,  105,  100, 1826, 3283,  454,    8,    2,   77,    6,\n",
      "           90,   11],\n",
      "        [   9,  217,    4,   14,   49, 3413,   31,    2,  118,   11,   14,   55,\n",
      "           36,   84,  487,   20,    5,    0,  102, 2881,    8,    2, 2165,  612,\n",
      "            9,   14,   38,    8,  113,   17, 8186,    0,  166,    4,    0,  419,\n",
      "            9,  217,   60,   20,    0,   16,    2,   84,    0,   98,   11,   14,\n",
      "           49,  500],\n",
      "        [ 337,  130,  655,    3,  130,  655,    3, 1121, 1063,   42,  104,    5,\n",
      "           55,   25, 2523,  248,    3,   56,    0,  205,   28,   55,    0,   13,\n",
      "         6285,  108, 1229,   31,  498,   24,   99,  171,  115,  872,  165,    6,\n",
      "            0,    0,    2,  227,  107, 1237,   29,    2,  173, 2052,   16,   36,\n",
      "            6,   73],\n",
      "        [   0,    0,    4,    0,   23,   29,    2,  410,    5,   58, 3005,  154,\n",
      "            8,    0,   43,   42,    2,  162, 7203,   18,    8,   55, 1158,   43,\n",
      "            3,   74,  135,   20,   68,    2, 5401,    0,   29,  225,   11,   14,\n",
      "         8107,  290,    0,   49,  329,  421,  216,   31,  502,    0,    4,    2,\n",
      "          422,    0]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 각 샘플은 더 이상 단어 시퀀스가 아니라 정수 시퀀스임을 볼 수 있다.\r\n",
    "- 중간에는 숫자 0이 존재하는데 이는 <unk\\> 토큰의 번호로 단어 집합에 포함되지 못한 단어들은 <unk\\>라는 토큰으로 변환되었다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <pad\\> 토큰이 사용되는 경우"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "batch = next(iter(train_loader)) # 첫번째 미니배치\r\n",
    "print(batch.text[0]) # 첫번째 미니배치 중 첫번째 샘플"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 189,   48,   30,    5,    2,  643,  350,   29,    2, 2831,   13, 6797,\n",
      "           2, 5482,  594,    3, 6097,   16,    3,    0,  594,  193,    2,  643,\n",
      "         822,   51,    0,    3,    0,    0,    4,    2,    0, 8416,    0,  867,\n",
      "           6,    3,  985,  189,    5,    0,  103,   13,  793,   30,    5,    2,\n",
      "          78, 3770])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit"
  },
  "interpreter": {
   "hash": "3019afab00293a1ae281e291b07de184983c2ba180a49ac4e0334f58b45935af"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}